{"cells":[{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-info\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b></font>\n","\n","–ü—Ä–∏–≤–µ—Ç –í–ª–∞–¥–∏—Å–ª–∞–≤! –ú–µ–Ω—è –∑–æ–≤—É—Ç –ú–∞—Ä–∞—Ç, –∏ —è –±—É–¥—É —Ç–≤–æ–∏–º —Ä–µ–≤—å—é–µ—Ä–æ–º. –°–ø–µ—à—É —Å–æ–æ–±—â–∏—Ç—å —á—Ç–æ –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —ç—Ç–∞–ø—ã –≤ —Ä–∞–±–æ—Ç–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã,  —Å –∑–∞–¥–∞—á–µ–π —Ç–µ–±–µ —É–¥–∞–ª–æ—Å—å —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è. –ü–æ –ø–æ–≤–æ–¥—É –æ–±—Ä–∞—â–µ–Ω–∏—è - –≤ IT —Å—Ñ–µ—Ä–µ –ø—Ä–∏–Ω—è—Ç–æ –æ–±—â–∞—Ç—å—Å—è –Ω–∞ ¬´—Ç—ã¬ª :) –ù–æ, –µ—Å–ª–∏ –ø—Ä–∏–≤—ã—á–Ω–µ–π –Ω–∞ ¬´–≤—ã¬ª, –¥–∞–π –∑–Ω–∞—Ç—å. –ö–∞–∫ —Ä–µ–≤—å—é–µ—Ä–∞ –º–æ—è –∑–∞–¥–∞—á–∞ –ø–æ–º–æ—á—å —Ç–µ–±–µ –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏, –¥–∞–≤ —Ö–æ—Ä–æ—à–∏–µ —Å–æ–≤–µ—Ç—ã. –Ø –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –ø–æ—Å–º–æ—Ç—Ä—é —Ç–≤–æ–π –∫–æ–¥, –æ–∑–Ω–∞–∫–æ–º–ª—é—Å—å —Å —Ç–≤–æ–∏–º–∏ –≤—ã–≤–æ–¥–∞–º–∏ –∏ –æ—Å—Ç–∞–≤–ª—é –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏. –ì–¥–µ —Ç–æ –º–æ–≥—É –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –Ω–µ–±–æ–ª—å—à–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ –∫–æ–¥–µ, –Ω–æ –Ω–µ–Ω–∞–≤—è–∑—á–∏–≤–æ. –ì–¥–µ –ø–æ—Ç—Ä–µ–±—É—é—Ç—Å—è —É—Ç–æ—á–Ω–µ–Ω–∏—è, —è –æ—Å—Ç–∞–≤–ª—é –º–Ω–æ–≥–æ –Ω–∞–≤–æ–¥—è—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤. –û–Ω–∏ –ø–æ–º–æ–≥—É—Ç —Ç–µ–±—è —Å –ø–æ–∏—Å–∫–æ–º –≤–µ—Ä–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è.\n","\n","–í—Å–µ –º–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ —Ä–∞–∑–º–µ—á–µ–Ω—ã –ø–æ —Ü–≤–µ—Ç–∞–º, –¥–ª—è –ª—É—á—à–µ–≥–æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è. \n","    \n","<div class=\"alert alert-success\">–ó–µ–ª–µ–Ω—ã–º —Ü–≤–µ—Ç–æ–º –∏ —Å–ª–æ–≤–æ–º ¬´–£—Å–ø–µ—Ö¬ª –æ—Ç–º–µ—á–µ–Ω—ã –æ—Å–æ–±–æ —É–¥–∞—á–Ω—ã–µ –∏ —ç–ª–µ–≥–∞–Ω—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–º–∏ —Ç—ã –º–æ–∂–µ—à—å –≥–æ—Ä–¥–∏—Ç—å—Å—è. </div>\n","        \n","<div class=\"alert alert-warning\">–ñ–µ–ª—Ç—ã–º –∏ –∑–Ω–∞—á–∫–æ–º —Å–ª–æ–≤–æ–º ¬´–°–æ–≤–µ—Ç¬ª, –ø–æ–º–µ—á–µ–Ω—ã —Ä–µ—à–µ–Ω–∏—è —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –±–æ–ª–µ–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ. –¢—ã –º–æ–∂–µ—à—å –Ω–∞–π—Ç–∏ –∏—Ö —Å—Ä–∞–∑—É –∏ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ–µ–∫—Ç, –∏–ª–∏ –æ—Ç–ª–æ–∂–∏—Ç—å —ç—Ç–æ –Ω–∞ –ø–æ—Ç–æ–º, –¥–ª—è –±—É–¥—É—â–∏—Ö –ø—Ä–æ–µ–∫—Ç–∞—Ö. –ü—Ä–æ–µ–∫—Ç –±—É–¥–µ—Ç –ø—Ä–∏–Ω—è—Ç –∏ –±–µ–∑ –∏—Ö –¥–æ—Ä–∞–±–æ—Ç–∫–∏. </div>\n","        \n","<div class=\"alert alert-danger\"> –ö—Ä–∞—Å–Ω—ã–º —Ü–≤–µ—Ç–æ–º –∏ –∑–Ω–∞—á–∫–æ–º —Å–ª–æ–≤–æ–º ¬´–û—à–∏–±–∫–∞¬ª –ø–æ–º–µ—á—É —Ç–≤–æ–∏ —Ä–µ—à–µ–Ω–∏—è, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–æ–∏—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ. –ü–æ—Å–ª–µ –∏—Ö –¥–æ—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–µ–∫—Ç –±—É–¥–µ—Ç –ø—Ä–∏–Ω—è—Ç. </div>\n","        \n","–ó–∞–ª–æ–≥ —É—Å–ø–µ—Ö–∞ - —Ä–∞–±–æ—Ç–∞ —Å–æ–æ–±—â–∞, –≤–∑–∞–∏–º–Ω–æ–µ —É–≤–∞–∂–µ–Ω–∏–µ –∏ —Ä–∞–±–æ—Ç–∞ –≤ –¥–∏–∞–ª–æ–≥–µ. –ü–æ—ç—Ç–æ–º—É, –ø–æ–º–µ—á–∞–π —Å–≤–æ–∏ –æ—Ç–≤–µ—Ç–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞ –º–æ–∏ —Ä–µ–ø–ª–∏–∫–∏ –∑–∞–º–µ—Ç–Ω—ã–º —Ü–≤–µ—Ç–æ–º –∏–ª–∏ –∫—É—Ä—Å–∏–≤–æ–º, —Ç–∞–∫ –º–Ω–µ –±—É–¥–µ—Ç –ª–µ–≥—á–µ –∏—Ö –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–µ –∏–∑–º–µ–Ω—è–π –∏ –Ω–µ —É–¥–∞–ª—è–π –º–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏. –í—Å–µ —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –±—ã—Å—Ç—Ä–µ–π.\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"background:aquamarine; padding:1rem;\">\n","  <h5>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞</h5>\n","  –°–ø–∞—Å–∏–±–æ –∑–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª–Ω–æ–µ —Ä–µ–≤—å—é! –ë—É–¥—É –∏–¥—Ç–∏ –æ—Ç –æ–±—Ä–∞—Ç–Ω–æ–≥–æ. –í –æ–ø–∏—Å–∞–Ω–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞ –µ—Å—Ç—å —É—Å–ª–æ–≤–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –º–µ—Ç—Ä–∏–∫–∏ F1 (0.75). –î–ª—è –Ω–∞—á–∞–ª–∞ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª–∑–æ–≤–∞—Ç—å LogisticRegression. –ù—É–∂–µ–Ω –ø–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö, –¥–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ (–≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å–∞–º—ã–π –¥–æ–ª–≥–∏–π –ø—Ä–æ—Ü–µ—Å—Å). –ú–∞–ª–µ–Ω—å–∫–∏–µ –≥—Ä—É–ø–∏—Ä–æ–≤–∫–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ–º–æ–≥—É—Ç –±—ã—Å—Ç—Ä–µ–µ –Ω–∞–π—Ç–∏ —Ñ–∏–Ω–∏—à, –Ω–æ –µ—Å–ª–∏ –≤ –≥—Ä—É–ø–∏—Ä–æ–≤–∫–µ –Ω–µ—Ç —Ö–æ—Ç—è–±—ã –æ–¥–Ω–æ–≥–æ –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞, –º–æ–¥–µ–ª—å —Ä—É–≥–∞–µ—Ç—Å—è. –Ø —Å–æ–≥–ª–∞—Ü–µ–Ω —á—Ç–æ –Ω–µ–ª—å–∑—è –¥–µ–ª–∞—Ç—å –∫–∞—Ç–æ–π –ø–µ—Ä–µ–±–æ—Ä –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—ç—Ç–æ –±—É–¥–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ). –ú–æ–π –∫–ª–∞—Å—Å BertDataPipeline –ø–æ–º–æ–≥–∞–µ—Ç –Ω–∞–º –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–ª–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –¥–µ—Ä–∂–∏—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ –º—ã —Ö–æ—Ç–∏–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏.\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞V2</b></font>\n","\n","\n","\n","–°–æ–≤–µ—Ç ü§î:\n","\n","\n","\n","–Ø —Ç–µ–±–µ –≤—Å—ë-—Ç–∞–∫–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 1500 train –∏ 500 test –ø—Ä–∏–º–µ—Ä–æ–≤.  –≠—Ç–æ —Ç–æ—Ç –æ–±—ä—ë–º, –∫–æ—Ç–æ—Ä—ã–π (–µ—Å–ª–∏ –µ—â—ë –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU) –±—ã—Å—Ç—Ä–æ, –∏  –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∏ –ø–æ–ª—É—á–∏—Ç—å –Ω–µ —Ä–∞–Ω–¥–æ–º–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.  –¢–æ —á—Ç–æ —Ç—ã –æ–±—É—á–∞–µ—à—å –Ω–∞ –≤—ã–±–æ—Ä–∫–µ 5, –∞ –ø–æ—Ç–æ–º —Å–º–æ—Ç—Ä–∏—à—å –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã–π —Å —Ç–∞–∫–∏–º –∂–µ —Ä–∞–∑–º–µ—Ä–æ–º - —ç—Ç–æ —á–∏—Å—Ç–æ–π –≤–æ–¥—ã —Ä–∞–Ω–¥–æ–º.  –¢–∞–∫ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∏—Ç—å\n"]},{"cell_type":"markdown","metadata":{},"source":["–ü—Ä–∏–≤–µ—Ç, —É –º–µ–Ω—è —Ç—Ä—É–¥–Ω–æ—Å—Ç–∏ –ø–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö, –Ω–µ–ø–æ–ª—É—á–∞–µ—Ç—Å—è –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∏—Ö —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å. –ú–Ω–µ –∫–∞–∂–µ—Ç—Å—è –ø—Ä–æ–±–ª–µ–º–∞ –≤ —Ç–∏–ø–µ –¥–∞–Ω–Ω—ã—Ö, –Ω–æ —è –Ω–µ —É–≤–µ—Ä–µ–Ω —á—Ç–æ –∏–º–µ–Ω–Ω–æ. –ï—Å–ª–∏ –º–æ–∂–Ω–æ –Ω–µ–º–æ–≥–æ —Ñ–∏–¥–±–µ–∫–∞, –°–ø–∞—Å–∏–±–æ!!!"]},{"cell_type":"markdown","metadata":{},"source":["# Project with BERT\n","reference (https://gist.github.com/albahnsen/b02d2183c93067e3f248a428430c970e)\n","# Project Goal (–¶–µ–ª—å –ü—Ä–æ–µ–∫—Ç–∞) \n","We are given a file with Twitter comments. Those comments are marked as positive/negative. We need to build a NLP model that will be able to predict the \"toxicity\" of comments with a minimum of 0.75 F1 Score. <br/><br/>\n","–ù–∞–º –¥–∞–Ω —Ñ–∞–π–ª —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ —Å –¢–≤–∏—Ç–µ—Ä–∞. –û–Ω–∏ –ø–æ–º–µ—á–µ–Ω–Ω—ã –∫–∞–∫ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ/–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ. –ù–∞–º —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ—Å—Ç—Ä–æ–∏—Ç—å NLP –º–æ–¥–µ–ª—å –∫–æ—Ç–æ—Ä–∞—è —Å–º–æ–∂–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π 0.75 –ø–æ F1 Score."]},{"cell_type":"markdown","metadata":{},"source":["# Project Outline (–ü–ª–∞–Ω –ü—Ä–æ–µ–∫—Ç–∞)\n","1. Project Goal (–¶–µ–ª—å –ü—Ä–æ–µ–∫—Ç–∞)\n","2. Outline  (–ü–ª–∞–Ω)\n","3. Libraries/Data Load  (–ë–∏–±–ª–µ–æ—Ç–µ–∫–∏/–í—ã–≥—Ä—É–∑–∫–∞ –î–∞–Ω–Ω—ã—Ö)\n","4. Data Inspection/Data Preprocessing  (–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ê–Ω–∞–ª–∏–∑/–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö)\n","5. Training Pipeline + Logistic Regression  (–§—É–Ω—Ü–∏—è –¥–ª—è –û–ø—Ç–∏–º–∏–∑—è—Ü–∏–∏ –û–±—É—á–µ–Ω–∏—è + –ü—Ä–æ—Å—Ç–æ–π –†–µ–≥—Ä–µ—Å—Å–æ—Ä)\n","6. Try Out the Training Pipeline with Other Logistic ML Models  (–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –î—Ä—É–≥–∏—Ö –ú–æ–¥–µ–ª–µ–π)\n","7. Results  (–í—ã–≤–æ–¥)\n"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-success\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b></font>\n","\n","–£—Å–ø–µ—Ö:\n","\n","–í—Å—Ç—É–ø–ª–µ–Ω–∏–µ –≤ —Ä–∞–±–æ—Ç—É –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ, —Ç–∞–∫ —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π —Å–º–æ—Ç—Ä–∏—Ç —Ç–≤–æ–π –ø—Ä–æ–µ–∫—Ç (–∏ –Ω–∞ —Ä–∞–±–æ—Ç–µ –≤ —Ç–æ–º —á–∏—Å–ª–µ) –±—É–¥–µ—Ç —Å—Ä–∞–∑—É –≤–≤–µ–¥–µ–Ω –≤ –∫—É—Ä—Å –¥–µ–ª–∞. \n","     \n","    \n"," \n","\n","<div class=\"alert alert-warning\">\n","\n","\n","–°–æ–≤–µ—Ç: \n","   \n","–í–æ–ø—Ä–æ—Å–∏–∫, –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–µ—à—å –æ—Ç–≤–µ—Ç–∏—Ç—å )\n","    \n","    \n","- –∞ –ø–æ—á–µ–º—É –ø–æ —Ç–≤–æ–µ–º—É –±—ã–ª–∞ –≤—ã–±—Ä–∞–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ f1? \n","    \n","    \n","- –∞ —á—Ç–æ –µ—Å–ª–∏ –±—ã –Ω–∞–º –±—ã–ª–æ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–∏ –Ω–∞ –∫–∞–∫—É—é –º–µ—Ç—Ä–∏–∫—É –º—ã –±—ã –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å?\n","    \n","    \n","- –∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –º—ã –º–æ–∂–µ–º –∏–∑–º–µ–Ω–∏—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –æ—à–∏–±–∫–∏ –≤ –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –æ–Ω–∞ –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞–ª–∞ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â—É—é –Ω–∞—Å –º–µ—Ç—Ä–∏–∫—É (accuracy, f1, precision, roc-auc –∏—Ç–ø)?    \n","\n","\n","\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["# Libraries/Data Load  (–ë–∏–±–ª–µ–æ—Ç–µ–∫–∏/–í—ã–≥—Ä—É–∑–∫–∞ –î–∞–Ω–Ω—ã—Ö)"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":false},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import os\n","from time import time\n","\n","import torch\n","from transformers import BertTokenizer, BertModel\n","from transformers import logging\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":false},"outputs":[],"source":["logging.set_verbosity_warning()\n","logging.set_verbosity_error()"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":false},"outputs":[],"source":["local_path = './datasets/toxic_comments.csv'\n","web_path = '/datasets/toxic_comments.csv'"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":false},"outputs":[],"source":["if os.path.exists(local_path):\n","  df = pd.read_csv(local_path)\n","elif os.path.exists(web_path):\n","  df = pd.read_csv(web_path)"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b></font>\n","\n","–°–æ–≤–µ—Ç: \n","\n","\n","–ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å - —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ —Å—Ç–æ–ª–±—Ü–∞  `Unnamed: 0` –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –º–æ–∂–Ω–æ —Ç–∞–∫:\n","\n","\n","    pd.read_csv(..., index_col=0)\n","\n","    \n","(`Unnamed: 0` –ø–æ—è–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏ –Ω–µ —Å–æ–≤—Å–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞)    \n","\n","\n","\n","\n","–ù–æ —ç—Ç–æ –º–µ–ª–æ—á—å,  –¥–∞–∂–µ –Ω–µ –Ω—É–∂–Ω–æ –Ω–∏—á–µ–≥–æ –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å. –ü—Ä–æ—Å—Ç–æ –∑–Ω–∞–π, —á—Ç–æ–±—ã —É–≤–∏–¥–µ–≤ —Ç–∞–∫–æ–µ –≤ —á—É–∂–æ–º –∫–æ–¥–µ –Ω–µ —É–¥–∏–≤–ª—è—Ç—å—Å—è —á—Ç–æ –±—ã —ç—Ç–æ –º–æ–≥–ª–æ –æ–∑–Ω–∞—á–∞—Ç—å"]},{"cell_type":"markdown","metadata":{},"source":["# Data Inspection/Data Preprocessing \n","# (–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ê–Ω–∞–ª–∏–∑/–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö)"]},{"cell_type":"markdown","metadata":{},"source":["Visual inspection of data showed that most of the comments are of english language. So far we can assume we will be using BERT tokenizer and BERT model \"bert-base-uncased\". <br/><br/>\n","–ü–æ—Å–ª–µ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö, —è—Å–Ω–æ —á—Ç–æ –º—ã —Ä–∞–±–æ—Ç–∞–µ–º —Å –∞–Ω–≥–ª–∏–π—Å–∫–∏–º–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏. –ó–Ω–∞—á–∏—Ç –±—É–¥–µ–º –∏—Å–ø–æ–ª–∑–æ–≤–∞—Ç—å 'bert-base-uncased'. "]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":false},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>toxic</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                               text  toxic\n","0           0  Explanation\\nWhy the edits made under my usern...      0\n","1           1  D'aww! He matches this background colour I'm s...      0\n","2           2  Hey man, I'm really not trying to edit war. It...      0\n","3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n","4           4  You, sir, are my hero. Any chance you remember...      0"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":false},"outputs":[],"source":["df = df.drop(columns=['Unnamed: 0'])"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":false},"outputs":[{"data":{"text/plain":["(159292, 2)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df.shape"]},{"cell_type":"markdown","metadata":{},"source":["It is also important to inspect the ratio of \"positive/negative\" target feature, we need to have a good balance while training the model 50/50. <br/><br/>\n","–ù–∞–¥–æ —Ç–æ–∂–µ –∏–º–º–µ—Ç—å –≤–≤–∏–¥—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è \"–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ/–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ\" –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è —Ü–µ–ª–µ–≥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞. –ù–∞–º –Ω—É–∂–Ω–æ —Ö–æ—Ä–æ—à–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ 50/50."]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["toxic to all comments ratio: 0.10161213369158527\n"]}],"source":["print(\"toxic to all comments ratio: {}\".format(df.toxic.sum()/df.shape[0]))"]},{"cell_type":"markdown","metadata":{},"source":["The ratio issue will be solved in the training pipeline. <br/><br/>\n","–ü—Ä–æ–±–ª–µ–º–º–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –±—É–¥–µ—Ç —Ä–µ—à–µ–Ω–Ω–∞ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è."]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-success\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b></font>\n","\n","–£—Å–ø–µ—Ö:\n","\n","–î–∞–Ω–Ω—ã–µ –∏–∑—É—á–µ–Ω—ã. –ù–µ–±–æ–ª—å—à–æ–π EDA –Ω–µ –ø–æ–º–µ—à–∞–µ—Ç, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ–µ–∫—Ç. \n","\n","–ó–¥–æ—Ä–æ–≤–æ —á—Ç–æ –æ–±—Ä–∞—Ç–∏–ª –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å\n","\n","<div class=\"alert alert-warning\">\n","\n","–°–æ–≤–µ—Ç: \n","\n","\n","\n","\n","- .sample() –≤–º–µ—Å—Ç–æ .head(), –≤–µ–¥—å –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫–∏–º —Ç–æ –æ–±—Ä–∞–∑–æ–º —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã, —Ç–æ —à–∞–Ω—Å—ã —É–≤–∏–¥–µ—Ç—å —á—Ç–æ —Ç–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–µ —á–µ—Ä–µ–∑ .sample —á—É—Ç—å –≤—ã—à–µ —á–µ–º —á–µ—Ä–µ–∑ .head (–∏–ª–∏ .tail)     \n","   \n","\n","\n","- —É–±—Ä–∞–≤ —Å—Ç–æ–ø—Å–ª–æ–≤–∞ –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞/[–æ–±–ª–∞–∫–æ —Å–ª–æ–≤](https://habr.com/ru/post/517410/) - —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –æ–±—â–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ —Ç–µ–º–∞—Ç–∏–∫–µ –∏ –æ –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ–º—ã—Ö —Å–ª–æ–≤–∞—Ö –≤ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∏ –Ω–µ—Ç–æ–∫—Å–∏—á–Ω—ã—Ö —Ç–≤–∏—Ç–∞—Ö –ö—Ä–æ–º–µ —Ç–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∏, —Ä–∏—Å—É–Ω–∫–∏ –¥–µ–ª–∞—é—Ç –ø—Ä–æ–µ–∫—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–µ–π"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"background:aquamarine; padding:1rem;\">\n","  <h5>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞</h5>\n","  –î–∞, —Ç–∞–∫–æ–π –∞–Ω–∞–ª–∏–∑ –±—ã–ª–æ –±—ã –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏. –ê –µ—Å–ª–∏ —É–±—Ä–∞—Ç—å —Å—Ç–æ–ø —Å–ª–æ–≤–∞, —ç—Ç–æ –Ω–µ –ø–æ–≤–ª–∏—è–µ—Ç –Ω–∞ BERT? –ú–Ω–µ –∫–∞–∂–µ—Ç—Å—è —á—Ç–æ —Å—Ç–æ–ø —Å–ª–æ–≤–∞ –º–æ–≥—É—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è –Ω–∞ –µ–º–±–µ–¥–∏–Ω–≥–∞—Ö.\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞V2</b></font>\n","\n","\n","\n","–°–æ–≤–µ—Ç ü§î:\n","\n","\n","\n","–£–±–∏—Ä–∞—Ç—å —Å—Ç–æ–ø —Å–ª–æ–≤–∞ –≤ —Å–ª—É—á–∞–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ë–µ—Ä—Ç–∞ –Ω–µ –Ω—É–∂–Ω–æ, –∏ –ª–µ–º–∞—Ç–∏–∑–∞—Ü–∏—è –Ω–µ –Ω—É–∂–Ω–∞.  –°—á–∏—Ç–∞–µ—Ç—Å—è —á—Ç–æ –±–µ—Ä–¥ —Å–ø–æ—Å–æ–±–µ–Ω —É—á–∏—Ç—ã–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤, –¥–∞ –∏ —Å–ª–æ–≤ —Å—Ç–æ—è—â–∏—Ö –≤ —Ä–∞–∑–Ω—ã—Ö –ø–∞–¥–µ–∂–∞—Ö.  –≠—Ç–æ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–æ–≥–æ —á—Ç–æ —è –ø—Ä–µ–¥–ª–æ–∂–∏–ª\n","\n","    \n","    \n","–ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ –±—ã–ª–æ –±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –æ–±–ª–∞–∫–æ, –∏ —Ç–∞–º —Å—Ä–∞–∑—É –∑–∞–¥–∞—Ç—å —Å—Ç–æ–ø —Å–ª–æ–≤–∞. –î–ª—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –º–æ–∂–Ω–æ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å —Ç–∞–∫.  –ú–æ–∂–µ—à—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö\n","    \n","    \n","    \n","    !/opt/conda/bin/python -m pip install wordcloud\n","    from wordcloud import WordCloud\n","    import matplotlib.pyplot as plt\n","\n","\n","    df_negative = df[df['toxic'] == 1]\n","    text_cloud = ' '.join(df_negative['text'])\n","    cloud = WordCloud(stopwords=stopwords, max_words=80, collocations=False).generate(text_cloud)\n","    plt.figure(figsize=(12,8))\n","    plt.imshow(cloud)\n","    plt.axis('off')\n","    plt.show()   \n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Training Pipeline + Logistic Regression  \n","# (–§—É–Ω—Ü–∏—è –¥–ª—è –û–ø—Ç–∏–º–∏–∑—è—Ü–∏–∏ –û–±—É—á–µ–Ω–∏—è + –ü—Ä–æ—Å—Ç–æ–π –†–µ–≥—Ä–µ—Å—Å–æ—Ä)"]},{"cell_type":"markdown","metadata":{},"source":["Optimizing training process - iterative model training and testing will let us know when to stop vectorizing the data.\n","In order to do that we need to create an instance of class that can hold vectorized train/test data, and when needed to perform those vectorizations in batch format. <br/><br/>\n","–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è - –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ–º–æ–∂–µ—Ç –Ω–∞–º –ø–æ–Ω—è—Ç—å –∫–æ–≥–¥–∞ –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Å—Ç–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. –î–ª—è —ç—Ç–æ–≥–æ –º—ã —Å–æ–∑–¥–∞–¥–∏–º –∫–ª–∞—Å—Å –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –¥–µ—Ä–∂–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –ø–æ –Ω–∞–¥–æ–±–Ω–æ—Å—Ç–∏, –≤ –∑–∞–≤–∏—Å—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞."]},{"cell_type":"markdown","metadata":{},"source":["--"]},{"cell_type":"markdown","metadata":{},"source":["We touched on the problem of positive/negative target feature ratio. Our class will handle it by first processing positive/negative data with 50/50 ratio, then if needed processing the rest of the data. <br/><br/>\n","–ü—Ä–æ–±–ª–µ–º–∞ —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ/–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ü–µ–ª–µ–≥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –±—É–¥–µ—Ç —Ä–µ—à–µ–Ω–Ω–∞ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: —Å–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º 50/50 –ø–æ—Ç–æ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–µ–º –∏ –ø–æ–¥–∞–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ."]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b></font>\n","\n","\n","\n","–°–æ–≤–µ—Ç ü§î:\n","\n","\n","\n","    –ù–∞–º –Ω—É–∂–Ω–æ —Ö–æ—Ä–æ—à–µ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ 50/50 –ü—Ä–æ–±–ª–µ–º–∞ —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ/–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ü–µ–ª–µ–≥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –±—É–¥–µ—Ç —Ä–µ—à–µ–Ω–Ω–∞ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º: —Å–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º 50/50 –ø–æ—Ç–æ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–µ–º –∏ –ø–æ–¥–∞–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n","\n","    \n","    \n","–ê –º–æ–∂–µ—à—å –æ–±—ä—è—Å–Ω–∏—Ç—å —ç—Ç—É —Ñ—Ä–∞–∑—É, –∏ —á—Ç–æ —Ç–µ–±–µ –¥–∞—ë—Ç —Ç–æ —á—Ç–æ –º—ã —Å–Ω–∞—á–∞–ª–∞ –±—É–¥–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ 50 –Ω–∞ 50?  –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ - —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –≤—ã–±–æ—Ä–∫—É –º—ã –º–æ–∂–µ–º –±—Ä–∞—Ç—å –∫–∞–∫—É—é —É–≥–æ–¥–Ω–æ, –Ω–æ  –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é (–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ) –∏ —Ç–µ—Å—Ç–æ–≤—É—é (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ) –¥–æ–ª–∂–Ω—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å    \n","    \n","    \n","–Ø –ø–æ—Å–º–æ—Ç—Ä–µ–ª –∫–∞–∫–æ–µ —É —Ç–µ–±—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ —É test, –ø–æ–ª—É—á–∏–ª 50 –Ω–∞ 50. –¢–∞–∫ –±—ã—Ç—å –Ω–µ –¥–æ–ª–∂–Ω–æ    "]},{"cell_type":"code","execution_count":48,"metadata":{"trusted":false},"outputs":[],"source":["def extracting_embeddings(text):\n","      tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","      model = BertModel.from_pretrained('bert-base-uncased')\n","      model.eval()\n","\n","      marked_text = \"[CLS] \" + text + \" [SEP]\"\n","\n","      # Tokenize our sentence with the BERT tokenizer\n","      tokenized_text = tokenizer.tokenize(marked_text)\n","      \n","      # Truncate the text\n","      truncated_text = tokenized_text[:512]\n","\n","      # Map the token strings to their vocabulary indices\n","      indexed_tokens = tokenizer.convert_tokens_to_ids(truncated_text)\n","\n","      # Pad the text\n","      indexed_tokens = indexed_tokens + [0] * (512 - len(indexed_tokens))\n","\n","      # Attention mask\n","      attention_mask = np.where(np.array(indexed_tokens) != 0, 1, 0)\n","\n","      # Convert inputs to PyTorch tensors\n","      tokens_tensor = torch.tensor([np.array(indexed_tokens)])\n","      attention_mask = torch.tensor([np.array(attention_mask)])\n","\n","      # Predict hidden states features for each layer\n","      with torch.no_grad():\n","        encoded_layers = model(tokens_tensor, attention_mask=attention_mask)\n","\n","      return np.concatenate(encoded_layers.last_hidden_state.numpy())"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","<div class=\"alert alert-warning\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b></font>\n","\n","\n","\n","–°–æ–≤–µ—Ç: \n","\n","–ï—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω–Ω—ã—Ö –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –≤ —Ç–µ–∫—Å—Ç–∞—Ö –∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–µ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ü–µ–ª–µ–π, –∏—Ö –º–æ–∂–Ω–æ –ø–æ–∏—Å–∫–∞—Ç—å –Ω–∞  –Ω–∞ huggingface. –¢–∞–º –∞–≤—Ç–æ—Ä—ã —É–∫–∞–∑—ã–≤–∞—é—Ç,–Ω–∞ –∫–∞–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —É—á–∏–ª–∏ –∏ –¥–ª—è –∫–∞–∫–∏—Ö —Ü–µ–ª–µ–π, –º–æ–∂–Ω–æ –ø–æ–∏—Å–∫–∞—Ç—å. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –Ω–∞—à–µ–≥–æ –¥–∞—Ç–æ—Å–µ—Ç–∞ –≤–æ–∑–º–æ–∂–Ω–æ –±—É–¥—É—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞ –º–æ–¥–µ–ª—å  toxic-bert. –°—É–¥—è –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –æ–Ω–∞ —Å–∞–º–æ–µ —Ç–æ )\n","\n","\n","\n","    \n","–ü–æ–¥–≥—Ä—É–∑–∏—Ç—å –º–æ–∂–Ω–æ —Ç–∞–∫\n","    \n","    \n","    model = transformers.AutoModel.from_pretrained('unitary/toxic-bert')\n","    tokenizer = transformers.AutoTokenizer.from_pretrained('unitary/toxic-bert')"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","<div class=\"alert alert-warning\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b></font>\n","\n","\n","\n","–°–æ–≤–µ—Ç ü§î:\n","\n","\n","    \n","–î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU. –≠—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤ Colab\n","\n","\n","\n","[Google Colab –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è](https://colab.research.google.com/)\n","    \n","[–ö–∞–∫ –≤–∫–ª—é—á–∏—Ç—å GPU –≤ Google Colab](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm)\n","    \n","[–ö–∞–∫ BERT –æ–±—É—á–∞—Ç—å –Ω–∞ GPU](https://huggingface.co/docs/transformers/performance)    "]},{"cell_type":"code","execution_count":57,"metadata":{"trusted":false},"outputs":[],"source":["hello = extracting_embeddings('hello')"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":false},"outputs":[],"source":["hello"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]},{"cell_type":"code","execution_count":52,"metadata":{"trusted":false},"outputs":[],"source":["class BertDataPipeline:\n","  def __init__(self, df, batch_size):\n","    self.batch_size = batch_size\n","    self.current_batch = 0\n","\n","    # optimize data for the ratio\n","    positive = df[df.toxic == 1]\n","    negative = df[df.toxic == 0]\n","    negative_in = negative.loc[:positive.shape[0]]\n","    negative_out = negative.loc[positive.shape[0]:]\n","    balanced = pd.concat([positive, negative_in], axis=0).reset_index(drop=True)\n","    balanced = balanced.sample(frac=1).reset_index(drop=True)\n","    self.df = pd.concat([balanced, negative_out], axis=0).reset_index(drop=True) # the balanced will come first\n","\n","    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    self.model = BertModel.from_pretrained('bert-base-uncased')\n","    self.model.eval()\n","\n","    self.train = pd.DataFrame({'vectorized':[], 'toxic':[]})\n","    self.test = pd.DataFrame({'vectorized':[], 'toxic':[]})\n","  \n","  def data_train_new(self):\n","    def extracting_embeddings(text):\n","      marked_text = \"[CLS] \" + text + \" [SEP]\"\n","\n","      # Tokenize our sentence with the BERT tokenizer\n","      tokenized_text = self.tokenizer.tokenize(marked_text)\n","      \n","      # Truncate the text\n","      truncated_text = tokenized_text[:512]\n","\n","      # Map the token strings to their vocabulary indices\n","      indexed_tokens = self.tokenizer.convert_tokens_to_ids(truncated_text)\n","\n","      # Pad the text\n","      indexed_tokens = indexed_tokens + [0] * (512 - len(indexed_tokens))\n","\n","      # Attention mask\n","      attention_mask = np.where(np.array(indexed_tokens) != 0, 1, 0)\n","\n","      # Convert inputs to PyTorch tensors\n","      tokens_tensor = torch.tensor([np.array(indexed_tokens)])\n","      attention_mask = torch.tensor([np.array(attention_mask)])\n","\n","      # Predict hidden states features for each layer\n","      with torch.no_grad():\n","        encoded_layers = self.model(tokens_tensor, attention_mask=attention_mask)\n","\n","      return encoded_layers[0][:,0,:].numpy()\n","      # return np.concatenate(encoded_layers.last_hidden_state.numpy())\n","\n","    # train data\n","    df_slice_train = self.df.loc[self.batch_size*self.current_batch:self.batch_size*(self.current_batch + 1)]\n","    self.current_batch += 1\n","\n","    x_train = df_slice_train.text\n","    x_train = x_train.apply(lambda x: extracting_embeddings(x))\n","    y_train = df_slice_train.toxic\n","\n","    self.train = pd.concat([self.train, pd.DataFrame({'vectorized': x_train, 'toxic': y_train})], axis=0).reset_index(drop=True)\n","    \n","    # test data\n","    df_slice_test = self.df.loc[self.batch_size*self.current_batch:self.batch_size*(self.current_batch + 1)]\n","    self.current_batch += 1\n","\n","    x_test = df_slice_test.text\n","    x_test = x_test.apply(lambda x: extracting_embeddings(x))\n","    y_test = df_slice_test.toxic\n","\n","    self.test = pd.concat([self.test, pd.DataFrame({'vectorized': x_test, 'toxic': y_test})], axis=0).reset_index(drop=True)\n","\n","    # return new training data\n","    return pd.DataFrame({'vectorized':x_train, 'toxic':y_train})\n","\n","  def data_train(self):\n","    return self.train\n","  \n","  def data_test(self):\n","    return self.test    "]},{"cell_type":"code","execution_count":53,"metadata":{"trusted":false},"outputs":[],"source":["def train_model(bertDataPipeline, model, limit_seconds, f1_score_min):\n","  time_start = time()\n","  f1_score_best = 0\n","  initial_learn = True\n","\n","  while (time() - time_start < limit_seconds):\n","    # exit condition - meet the score requirements, or time limit ^^^\n","    # (also if we run out of data - unlikely, not to worry about, it would also take too long to vectorize all that data)\n","\n","    if f1_score_best >= f1_score_min:\n","      break\n","\n","    if initial_learn:\n","      initial_learn = False\n","      if bertDataPipeline.data_train().shape[0] > 0:\n","        df_train = bertDataPipeline.data_train()\n","        print(df_train.head())\n","        model.fit(df_train.vectorized, df_train.toxic)\n","        df_test = bertDataPipeline.data_test()\n","        z_test = model.predict(df_test.vectorized)\n","        y_test = df_test.toxic\n","        f1_score_current = f1_score(y_test, z_test)\n","        if f1_score_current > f1_score_best:\n","          f1_score_best = f1_score_current\n","    else:\n","      df_train = bertDataPipeline.data_train_new()\n","      print(df_train.head())\n","      model.fit(df_train.vectorized, df_train.toxic)\n","      df_test = bertDataPipeline.data_test()\n","      z_test = model.predict(df_test.vectorized)\n","      y_test = df_test.toxic\n","      f1_score_current = f1_score(y_test, z_test)\n","      if f1_score_current > f1_score_best:\n","        f1_score_best = f1_score_current\n","  \n","  print('best f1_score {}'.format(f1_score_best))\n","  print('time to train {}s'.format(time() - time_start))"]},{"cell_type":"code","execution_count":54,"metadata":{"trusted":false},"outputs":[],"source":["bertDataPipeline = BertDataPipeline(df, 5)"]},{"cell_type":"code","execution_count":55,"metadata":{"trusted":false},"outputs":[],"source":["logistic_linear_model = LogisticRegression(max_iter=1000)"]},{"cell_type":"code","execution_count":56,"metadata":{"scrolled":true,"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["                                          vectorized  toxic\n","0  [[-0.107602015, 0.42301303, 0.07229452, -0.567...      0\n","1  [[0.19171593, -0.09149636, -0.33626673, -0.082...      1\n","2  [[0.117458366, -0.10120446, -0.13708822, -0.56...      1\n","3  [[-0.9321038, 0.0019665472, -0.0461475, -0.515...      0\n","4  [[-0.15104553, 0.081879444, -0.080407724, -0.5...      1\n"]},{"ename":"ValueError","evalue":"setting an array element with a sequence.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/var/folders/sc/09svshb15mg04p30bkrrq31w0000gn/T/ipykernel_49508/1282537418.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbertDataPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogistic_linear_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/var/folders/sc/09svshb15mg04p30bkrrq31w0000gn/T/ipykernel_49508/2462268174.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(bertDataPipeline, model, limit_seconds, f1_score_min)\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertDataPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoxic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbertDataPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mz_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1506\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m   1509\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n","\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    870\u001b[0m               dtype='datetime64[ns]')\n\u001b[1;32m    871\u001b[0m         \"\"\"\n\u001b[0;32m--> 872\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."]}],"source":["train_model(bertDataPipeline, logistic_linear_model, 120, 0.75)"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b></font>\n","\n","\n","\n","–°–æ–≤–µ—Ç ü§î:\n","\n","\n","–ê –ø–æ—á–µ–º—É —Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª\n","    \n","    np.concatenate(encoded_layers.last_hidden_state.numpy())\n","\n","    \n","–ê –Ω–µ:\n","    \n","   encoded_layers[0][:,0,:].numpy()\n","    \n","    \n","    \n","–¢–≤–æ–∏ —ç–º–±–µ–¥–∏–Ω–≥–µ - —ç—Ç–æ  —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π    512–Ω–∞768, —á—Ç–æ —Å —ç—Ç–∏–º –¥–µ–ª–∞—Ç—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫–æ—Ç–æ—Ä—É—é —Ç—ã –ø–æ–¥–∞–ª —ç—Ç–æ –Ω–∞ –≤—Ö–æ–¥? \n","    \n","    \n","–ê    \n","    \n","    encoded_layers[0][:,0,:].numpy()    \n","    \n","    \n","—ç—Ç–æ 1–Ω–∞768, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å —ç—Ç–∏–º —Ä–∞–±–æ—Ç–∞—Ç—å –º–æ–∂–µ—Ç     \n","    \n","    \n","–ú–æ–∂–µ—Ç —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–¥ –∏–∑ —Ç—Ä–µ–Ω–∞–∂—ë—Ä–∞?    "]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"background:aquamarine; padding:1rem;\">\n","  <h5>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞</h5>\n","  –Ø –¥—É–º–∞–ª —á—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–∫—Ä–µ—Ç—ã–π —Å–ª–æ–π —ç—Ç–æ –∏ –µ—Å—Ç—å embeddings. –í —Ç—Ä–µ–Ω–∞–∂–µ—Ä–µ –æ—Å–æ–±–æ –Ω–µ –æ–±—å—è—Å–Ω—è–ª–∏ —á—Ç–æ –º—ã –ø–æ–ª—É—á–∞–µ–º –Ω–∞ –≤—ã—Ö–æ–¥–µ –∏–∑ BERT. –ú–µ–Ω—è —ç—Ç–∞ —Ç–µ–º–∞ –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç, —è –Ω–µ –º–æ–≥—É –ø—Ä–æ—Å—Ç–æ copy paste. –ò –≤–µ–¥—å embeddings —ç—Ç–æ –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤ —Å–æ —Å–º—ã—Å–ª–æ–º, —á—Ç–æ –∫–∞–∫ —Ä–∞–∑ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ–¥ –∑–∞–¥–∞—á—É.\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞V2</b></font>\n","\n","\n","\n","–°–æ–≤–µ—Ç ü§î:\n","\n","–≠—Ç–æ –∏ –µ—Å—Ç—å embedding —Å–ª–æ–≤, –∏—Ö –∫–æ–Ω–µ—á–Ω–æ –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å, –Ω–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å.  –Ø —Ç–µ–±–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª –ø—Ä–∏–º–µ–Ω–∏—Ç—å embedding –≤—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–∞–∫ –¥–µ–ª–∞—é—Ç —á–∞—â–µ,  –∏ —Ç–∞–∫ –æ–±—ã—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ª—É—á—à–µ    \n","    \n","–≠—Ç–æ –≤—Å—ë –≥—É–≥–ª–∏—Ç—Å—è.  –ù–∞–ø—Ä–∏–º–µ—Ä –º–æ–∂–µ—à—å –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —ç—Ç–æ [–≤–∏–¥–µ–æ](https://www.youtube.com/watch?v=Z1J3sTJYIcc&list=PLEwK9wdS5g0qksxWxtE5c2KuFkIfUXe3i&index=15), –ø—Ä–∏–º–µ—Ä–Ω–æ c —Ç—Ä–∏–¥—Ü–∞—Ç—å –≤—Ç–æ—Ä–æ–π –º–∏–Ω—É—Ç—ã –æ  —Ç–æ–º —á—Ç–æ –º—ã –ø–æ–ª—É—á–∞–µ–º –Ω–∞ –≤—ã—Ö–æ–¥–µ –∏–∑ –º–æ–¥–µ–ª–∏ –ë–µ—Ä—Ç\n"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b></font>\n","\n","\n","\n","–°–æ–≤–µ—Ç ü§î:\n","\n","\n","\n","–í–æ–æ–±—â–µ —Ö–æ—Ç–µ–ª–æ—Å—å –ø–æ–±–æ–ª—å—à–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –∫ –∫–æ–¥—É.  –ü–æ–∫–∞ —á—Ç–æ —è –≤–∏–∂—É –Ω–µ–ø–æ–Ω—è—Ç–Ω—É—é –º–Ω–µ –ª–æ–≥–∏–∫—É —Å –æ–±—É—á–µ–Ω–∏–µ–º, –∫–æ–≥–¥–∞ —Ç—ã –Ω–∞ 5 –ø—Ä–∏–º–µ—Ä–∞—Ö (—Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞) –æ–±—É—á–∞–µ—à—å –º–æ–¥–µ–ª—å, –∏ —Å—Ä–∞–∑—É –¥–µ–ª–∞–µ—à—å –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã–π –∏ –≤—Å—ë —ç—Ç–æ –≤ —Ü–∏–∫–ª–µ, –¥–≤–∏–≥–∞—è—Å—å –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É, —Å–æ —Å—Ç–æ–ø–æ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–ª–∏ –∫–æ–≥–¥–∞ –ú–µ—Ç—Ä–∏–∫–∞ –¥–æ—Å—Ç–∏–≥–Ω–µ—Ç > 0.75. –ê –º–æ–∂–µ—à—å –æ–±—ä—è—Å–Ω–∏—Ç—å —á—Ç–æ –º—ã –≤ —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ –º–æ–∂–µ–º –æ–±—É—á–∏—Ç—å? –ù–∞ –ø—è—Ç–∏ –ø—Ä–∏–º–µ—Ä–∞—Ö –º—ã –º–æ–∂–µ–º –ø–æ–ª—É—á–∏—Ç—å —á—Ç–æ —É–≥–æ–¥–Ω–æ, –Ω–æ —ç—Ç–æ –Ω–µ –∑–Ω–∞—á–∏—Ç —á—Ç–æ —É –Ω–∞—Å –±—É–¥–µ—Ç —Ö–æ—Ä–æ—à–∞—è –º–æ–¥–µ–ª—å. –ù–∞–º –Ω—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (–°–ª—É—á–∞–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ë–µ—Ä—Ç–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ –ø–æ—Ä–µ–∑–∞—Ç—å –≤—ã–±–æ—Ä–∫—É –¥–æ 1500 –Ω–∞ train b 500 –Ω–∞ test), –∞ –±–∞—Ç—á–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–æ–ª–Ω—è—Ç—å –ø–∞–º—è—Ç—å. –ü–∏—à—É –≤—Å–µ —Å–≤–æ–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∂—ë–ª—Ç—ã–º, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–µ —Å–æ–≤—Å–µ–º —É–≤–µ—Ä–µ–Ω —á—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–æ–Ω—è–ª –¢–≤–æ—é –∏–¥–µ—é.  –ñ–¥—É –ø–æ—è—Å–Ω–µ–Ω–∏–π.  –í–æ–∑–º–æ–∂–Ω–æ —Å—Ç–æ–∏—Ç –≤—Å—ë-—Ç–∞–∫–∏ —Å–¥–µ–ª–∞—Ç—å –∫–∞–∫ —Ç—Ä–µ–Ω–∞–∂—ë—Ä–µ, —Ö–æ—Ç—è –±—ã –¥–ª—è –Ω–∞—á–∞–ª–∞.  –ß—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –∑–∞—Ç—è–≥–∏–≤–∞–ª–∞—Å—å –¥–æ –≤—Ç–æ—Ä–æ–≥–æ –ü—Ä–∏—à–µ—Å—Ç–≤–∏—è –•—Ä–∏—Å—Ç–∞\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","<div class=\"alert alert-info\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞</b></font>\n","\n","\n","–°–ø–∞—Å–∏–±–æ –∑–∞ —Ä–∞–±–æ—Ç—É –í–ª–∞–¥–∏—Å–ª–∞–≤!\n","    \n","    \n","–ê —É –º–µ–Ω—è —É —Å–∞–º–æ–≥–æ –º–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤.  –í–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ —Å—Ç–æ–∏—Ç –ø—Ä–æ–≥–Ω–∞—Ç—å –∫–æ–¥ –±–µ–∑ –æ–æ–ø. –ù–µ —Ö–æ—Ç–µ–ª–æ—Å—å –±—ã –ø–æ–ª—É—á–∏—Ç—å –ø–æ—è—Å–Ω–µ–Ω–∏–µ –æ –ª–æ–≥–∏–∫–µ. \n","    \n","    \n","–ü–æ–∫–∞ —è –≤–∏–∂—É —á—Ç–æ\n","    \n","    \n","- –ù–µ–ø–æ–Ω—è—Ç–∫–∏ —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º (–ø–æ—á–µ–º—É —É —Ç–µ–±—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –∫–ª–∞—Å—Å–∞ 50 –Ω–∞ 50)  \n","    \n","    \n","    \n","- –õ–æ–≥–∏–∫–∞ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è (–ü–æ—á–µ–º—É —É —Ç–µ–±—è –æ–±—É—á–µ–Ω–∏–µ –∏–¥—ë—Ç –Ω–∞ –ø—è—Ç–∏ –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö, –ø–æ—Å–ª–µ —á–µ–≥–æ —Ç—ã –¥–µ–ª–∞–µ—à—å –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –ø—è—Ç–∏, —è –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—à—å –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ 0,75)\n","    \n","    \n","    \n","- –ü–æ—á–µ–º—É —Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å —Å–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏.  –ö–∞–∫ –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –∫–æ—Ç–æ—Ä—É—é —Ç—ã –ø–æ–¥–∞—ë—à—å     512–Ω–∞768 —Ä–∞–±–æ—Ç–∞—Ç—å —Å —ç—Ç–∏–º?\n","    \n","    \n","    \n","–í –æ–±—â–µ–º —Ç–≤–æ–π –ø—Ä–æ–µ–∫—Ç –≤—ã—Å—à–µ–π —Å—Ç–µ–ø–µ–Ω–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π, –∏ —è –¥–∞—é –¥–æ–ª–∂–Ω–æ–µ —á—Ç–æ —Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –∫–ª–∞—Å—Å—ã, –Ω–æ –ø–æ–∫–∞ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ –∫–∞–∫ –∫ –Ω–µ–º—É –ø–æ–¥—Å—Ç—É–ø–∞—Ç—å—Å—è"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"background:aquamarine; padding:1rem;\">\n","  <h5>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å—Ç—É–¥–µ–Ω—Ç–∞</h5>\n","  –ï—Å–ª–∏ embeddings –Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª–∑–æ–≤–∞—Ç—å —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∏–ª–∏ –≤—Ä—è–¥–ª–∏ —Ç–∞–∫ –ø–æ–ª—É—á–∏—Ç—Å—è, —è –≤—Å–µ –∏—Å–ø—Ä–∞–≤–ª—é.\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["\n","<div class=\"alert alert-info\">\n","<font size=\"5\"><b>–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Ä–µ–≤—å—é–µ—Ä–∞V2</b></font>\n","\n","\n","    \n","–í–ª–∞–¥–∏—Å–ª–∞–≤ –Ω–µ –∑–Ω–∞—é –∫–∞–∫ —É —Ç–µ–±—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º, –Ω–æ –¥—É–º–∞—é —Å—Ç–æ–∏—Ç —É—Å–∫–æ—Ä–∏—Ç—å—Å—è.  –Ø –≤—Å—ë-—Ç–∞–∫–∏ –ø—Ä–µ–¥–ª–∞–≥–∞—é —Ç–µ–±–µ —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–ø–∏—Å–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–æ–µ–∫—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ë–µ—Ä—Ç (–í–æ–∑—å–º–∏ –∑–∞ –æ—Å–Ω–æ–≤—É –∫–æ–¥ –∏–∑ —Ç—Ä–µ–Ω–∞–∂—ë—Ä–µ, –∏ –º–æ–∏ —Å–æ–≤–µ—Ç—ã –ø–æ –≤—ã–±–æ—Ä—É –º–æ–¥–µ–ª–∏, –∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—é), –∞ –∑–∞—Ç–µ–º –∫–æ–≥–¥–∞ —Ç—ã –±—É–¥–µ—à—å —Å–ø–æ–∫–æ–µ–Ω –ø–æ –ø–æ–≤–æ–¥—É –¥–µ–¥–ª–∞–π–Ω–∞ –º–æ–∂–µ—à—å –ø–æ–∫—Ä–µ–∞—Ç–∏–≤–∏—Ç—å    "]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"ExecuteTimeLog":[{"duration":57,"start_time":"2023-04-25T06:42:46.781Z"},{"duration":3,"start_time":"2023-04-25T06:42:56.967Z"}],"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"vscode":{"interpreter":{"hash":"45703e4c639f55edd07b7934093f007f2bf0e2096adf152af19818193a1ac5d8"}}},"nbformat":4,"nbformat_minor":2}
